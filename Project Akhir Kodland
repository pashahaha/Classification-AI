{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOqge+ram0vw6dYeXmLrIg5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"fymiUjWUFvag"},"outputs":[],"source":["!unzip /content/converted_keras.zip"]},{"cell_type":"code","source":["!pip install Pillow==9.1.0"],"metadata":{"id":"lberozmAHn3Y","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import files\n","from PIL import Image\n","\n","path = files.upload()\n","path = list(path.keys())[0]\n","Image = Image.open(path)"],"metadata":{"id":"Wdnh8y6dLOg6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install tensorflow==2.12"],"metadata":{"id":"kwNb3IH8DsxC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from keras.models import load_model  # TensorFlow is required for Keras to work\n","from PIL import Image, ImageOps  # Install pillow instead of PIL\n","import numpy as np\n","\n","# Disable scientific notation for clarity\n","np.set_printoptions(suppress=True)\n","\n","# Load the model\n","model = load_model(\"keras_model.h5\", compile=False)\n","\n","# Load the labels\n","class_names = open(\"labels.txt\", \"r\").readlines()\n","\n","# Create the array of the right shape to feed into the keras model\n","# The 'length' or number of images you can put into the array is\n","# determined by the first position in the shape tuple, in this case 1\n","data = np.ndarray(shape=(1, 224, 224, 3), dtype=np.float32)\n","\n","# Replace this with the path to your image\n","image = Image.open(path).convert(\"RGB\")\n","\n","# resizing the image to be at least 224x224 and then cropping from the center\n","size = (224, 224)\n","image = ImageOps.fit(image, size, Image.Resampling.LANCZOS)\n","\n","# turn the image into a numpy array\n","image_array = np.asarray(image)\n","\n","# Normalize the image\n","normalized_image_array = (image_array.astype(np.float32) / 127.5) - 1\n","\n","# Load the image into the array\n","data[0] = normalized_image_array\n","\n","# Predicts the model\n","prediction = model.predict(data)\n","index = np.argmax(prediction)\n","class_name = class_names[index]\n","confidence_score = prediction[0][index]\n","\n","# Print prediction and confidence score\n","print(\"Class:\", class_name[2:], end=\"\")\n","print(\"Confidence Score:\", confidence_score)"],"metadata":{"id":"rTRqgTE6M38Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def detect_musim(path, model, labels):\n","  # Disable scientific notation for clarity\n","  np.set_printoptions(suppress=True)\n","\n","  # Load the model\n","  model = load_model(model, compile=False)\n","\n","  # Load the labels\n","  class_names = open(labels, \"r\").readlines()\n","\n","  # Create the array of the right shape to feed into the keras model\n","  # The 'length' or number of images you can put into the array is\n","  # determined by the first position in the shape tuple, in this case 1\n","  data = np.ndarray(shape=(1, 224, 224, 3), dtype=np.float32)\n","\n","  # Replace this with the path to your image\n","  image = Image.open(path).convert(\"RGB\")\n","\n","  # resizing the image to be at least 224x224 and then cropping from the center\n","  size = (224, 224)\n","  image = ImageOps.fit(image, size, Image.Resampling.LANCZOS)\n","\n","  # turn the image into a numpy array\n","  image_array = np.asarray(image)\n","\n","  # Normalize the image\n","  normalized_image_array = (image_array.astype(np.float32) / 127.5) - 1\n","\n","  # Load the image into the array\n","  data[0] = normalized_image_array\n","\n","  # Predicts the model\n","  prediction = model.predict(data)\n","  index = np.argmax(prediction)\n","  class_name = class_names[index]\n","  confidence_score = prediction[0][index]\n","\n","  # Print prediction and confidence score\n","  print(\"Class:\", class_name[2:], end=\"\")\n","  print(\"Confidence Score:\", confidence_score)"],"metadata":{"id":"uJlKyR8-OfmM"},"execution_count":null,"outputs":[]}]}